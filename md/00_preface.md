# 前言

关于本书

要真正理解大数据，需要一些历史背景是有帮助的。大约在2001年，Gartner给出了大数据的定义：Big data is data that
contains greater variety arriving in increasing volumes and with
ever-higher
velocity。其意思是大数据是这样的数据，随着其不断增加的容量和更高的速度，数据类型具有更大的多样性，这就是所谓的3V（Variety、Volume和Velocity）。简而言之，大数据是更大、更复杂的数据集，尤其是来自更多的新数据源。这些数据集非常庞大，传统的数据处理软件无法管理它们。但是，这些大量的数据可以用来解决之前无法解决的业务问题。

尽管大数据本身的概念相对较新，但大数据集的起源可追溯到20世纪60年代和70年代，当数据世界刚刚起步时，出现第一个数据中心和关系数据库的发展。大约在2005年，人们开始意识到通过Facebook、YouTube和其他在线服务产生的用户数量。
Hadoop是在同一年开发的，是专门为存储和分析大数据集而创建的开源框架，NoSQL在这段时间也开始流行起来。像Hadoop这样的开源框架，例如Spark的发展对于大数据的发展至关重要，因为它们使得大数据更容易处理并且更便宜地存储。在那之后的几年中，数据量会急剧上升，用户仍然在生成大量的数据。随着物联网（IoT）的出现，更多的物体和设备连接到互联网，收集关于客户使用模式和产品性能的数据，以及机器学习的出现产生了更多的数据。虽然大数据已经走了很长的路，但其的实用性只是刚刚开始。

我们编写本书的目的在于介绍大数据发展趋势和基于Spark的生态环境，全面系统地提供Spark开发的基础知识，提供基于Docker容器开发环境和编程实例。在本书的编写中把Spark的基础知识与教学经验和学习体会结合起来，希望可以引导Spark技术学习者快速入门，系统的掌握Spark的编程技术。

本书围绕Spark技术开发，共分11章：

第1章介绍了Spark的生态环境，其中包括关键技术、Spark技术特征、编程语言、虚拟环境和HBase等，并且提供了实际的操作方法，这部分技术和知识会在后面的章节详细说明，并加以应用。

第2章Spark提供了访问和轻松存储数据的能力，可以在许多文件系统上运行，例如HDFS、Hbase、MongoDB、Cassandra，也可以将数据存储在其本地文件系统中，本章介绍了这些数据格式的基本概念。另外，通过本章的学习理解RDD的基本概念，掌握Spark程序的基本结构以及基础编程、编译和运行过程，学会Spark开发环境的配置。

第3章重点讲解了键值对RDD功能，学习如何创建键值对RDD和进行键值对RDD的相关操作，如Spark
RDD中的转换和动作。这里的转换操作包括groupByKey、reduceByKey、join、leftOuterJoin和rightOuterJoin等，而countByKey等是针对键值对RDD的动作。在本章中还讨论Spark中的分区，可以将其定义为大数据集的划分，并将它们作为整个群集中的多个部分存储。

第4章本章首先介绍了DataFrame和DataSet相关API的概念，并且区分了两种接口API的特征，然后介绍了RDD、DataFrame和DataSet三种数据结构的转换，最后介绍了结构化数据的操作方法。

第5章学习了Spark数据流的基础知识；流式传输的需求、体系结构和传输方式；还了解了Spark数据流的输入源的类型以及各种数据流操作。

第6章学习了Spark中GraphX API以及属性图的概念；介绍图形运算符和Pregel
API。另外，还学习了GraphX的方法和GraphX API的用例。

第7章学习了机器学习基本算法，Spark MLlib 的数据类型和API，最后介绍了机器学习的实用程序。

第8章学习了完成特征工程的工具集。根据具体问题，执行特征选择有很多不同的选项。如TF-IDF，Word 2
Vec和Vectorizers用于文本分析问题，适合文本的特征选择；对于特征转换，可以使用各种缩放器、编码器和离散器；对于向量的子集，可以使用VectorSlicer和Chi-Square
Selector，它们使用标记的分类特征来决定选择哪些特征。

第9章总结和各种机器学习算法，分类和回归：支持向量机，逻辑回归，线性回归，决策树，朴素贝叶斯分类；协作过滤技术包括交替最小二乘（ALS）；聚类分析方法包括k均值和潜在狄利克雷分配（LDA），等等。

第10章讲述如何设置一个完整的开发环境来开发和调试Spark应用程序。本章使用Scala和Java作为开发语言，sbt和Maven作为构建工具，讲述如何使用管理依赖项、如何打包和部署Spark应用程序。另外还介绍了Spark应用程序的几种部署模式。

第11章提供有关如何调整Apache
Spark作业的相关信息，性能调优介绍、Spark序列化库（如Java序列化和Kryo序列化）、Spark内存调优，还学习了Spark数据结构调优，Spark数据区域性和垃圾收集调优。

读者对象

本书是Spark技术开发的基础类书籍，通过本书的学习读者可以牢固的掌握Spark编程技术的基本概念、原理和编程方法，为实际的业务开发提供参考。

致谢

在本书的写作过程中,得到了很多人士的悉心帮助，在此谨向给予本书帮助的诸位及本书所参考的官方网站和网站社区表示诚挚的感谢。

特别感谢对外经济贸易大学信息学院，为本书的教学和实践提供了支持平台。

特别感谢软件工程师伊凡，他对本书中的代码进行了整理和调试，并提出了宝贵的意见。
